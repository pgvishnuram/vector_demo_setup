log_schema:
  timestamp_key: "@timestamp"
data_dir: ./vector/data
api:
  enabled: true
sources:
  generate_syslog:
    type: file
    include:
      - "./vector/data/*.log"
    #read_from: beginning

transforms:
  transform_syslog:
    type: remap
    inputs:
      - generate_syslog
    source: |
      .component = "${COMPONENT:--}"
      .workspace = "${WORKSPACE:--}"
      .release = "${RELEASE:--}"
      .date_nano = parse_timestamp!(.@timestamp, format: "%Y-%m-%dT%H:%M:%S.%f%Z")

  transform_task_log:
    type: remap
    inputs:
      - transform_syslog
    source: |-
      # Parse Syslog input. The "!" means that the script should abort on error.
      . = parse_json!(.message)
      .@timestamp = parse_timestamp(.timestamp, "%Y-%m-%dT%H:%M:%S%Z") ?? now()
      .check_log_id = exists(.log_id)
      if .check_log_id != true {
      .log_id = join!([.dag_id, .task_id, .execution_date, .try_number], "_")
      }
      .offset = to_int(now()) * 1000000000 + to_unix_timestamp(now()) * 1000000

sinks:
  out:
    type: elasticsearch
    inputs:
      - transform_task_log
    mode: bulk
    compression: none
    endpoint: "http://localhost:9200"
    #auth:
    #  strategy: "basic"
    #  user: astro001
    #  password : KNF3r6YFQsgV6FHoUY4kpkj6kZg60KYJ
    bulk:
      index: "vector.${RELEASE:--}.%Y-%m-%d"
      action: create

  tapdata:
    type: blackhole
    inputs:
      - transform_task_log
